#+title: Measuring evaluation times

This example involves simulating trees of varying sizes and then timing how long
it takes to evaluate the likelihood function on the resulting data set using our
approximation. There are instructions for timing the evaluation times using the
algorithm described by Manceau /et al/ (2020) included here as well. We refer to
that as the =popsize-distribution= since that is the name of the repository in
which it is implemented.

** Running the BDSCOD profiling

The profiling of the Haskell code is done using the =criterion= profiling
package which provides a CLI (which is why there is long =stack exec= command)
used in the following =main.sh= script.

#+BEGIN_SRC sh :tangle main.sh
Rscript src/make-json-config.R

stack build 

rm -f out/*
rm -f fibber.html 
rm -f fobber.csv 

stack exec -- timing-evaluation --output fibber.html --csv fobber.csv --time-limit 5 
Rscript src/prepare-simulations-for-popsize.R 
#+END_SRC

The files =fibber.html= and =fobber.csv= are used by =criterion= to write output
to. At this point you can open =fibber.html= in a browser to see the results.
The next steps are used to do the analogous timing in python.

** Running Manceau's approximation

*REMEMBER TO SOURCE THE VIRTUAL ENVIRONMENT FIRST!*

To compute the evaluation times for the =popsize-distribuion= function, you need
to clone the correct repository into the current directory and then set up the
necessary python environment (there is a =requirements.txt= provided). Then
after running the BDSCOD timing, you need to adjust the simulation files and run
=run-python-timing.sh= from withing =popsize-distribution= to generate the
timing results.

The output of =pip freeze= for the virtual environment being used is here, i.e.,
the =requirements.txt= is given below.

#+begin_src sh
python3 -m venv venv
source venv/bin/activate
pip install -U pip
pip install -r requirements.txt
#+end_src

where the =requirements.txt= is as follows.

#+BEGIN_SRC :tangle requirements.txt
cycler==0.10.0
Cython==0.29.21
ete3==3.1.1
kiwisolver==1.1.0
matplotlib==3.0.3
numpy==1.18.5
pkg-resources==0.0.0
pyparsing==2.4.7
python-dateutil==2.8.1
scipy==1.4.1
six==1.15.0
#+END_SRC

To actually run the this, you need to clone the =popsize-distribution=
repository and copy over a couple of helper scripts as follows. The commit for
=popsize-distribution= I used is =5223bd2812423c9be0c3086a30df25699cedcdda=.

#+BEGIN_SRC sh
git clone https://gitlab.com/MMarc/popsize-distribution.git

cd popsize-distribution 

cp ../timing.py .
cp ../run-python-timing.sh

./run-python-timing.sh
#+END_SRC

** Running

*REMEMBER TO SOURCE THE VIRTUAL ENVIRONMENT FIRST!*

The profiling of the haskell code is done with =criterion= which provides a CLI
which is why there is long =stack exec= command.

#+BEGIN_SRC sh :tangle main.sh
stack clean 
stack build 
rm out/*
rm fibber.html 
rm fobber.csv 
stack exec -- timing-evaluation --output fibber.html --csv fobber.csv --time-limit 5 
Rscript src/prepare-simulations-for-popsize.R 
#+END_SRC

At this point you can open =fibber.html= in a browser to see the report
generated by criterion of how the profiling went. The next steps are used to do
the analogous timing in python.

#+BEGIN_SRC sh
source venv/bin/activate
cd popsize-distribution 
./run-python-timing.sh
cd ../ 
deactivate
#+END_SRC

The =run-python-timing.sh= script is something that has been added to
=popsize-distribution=. It loops over the reformatted files and runs the
=timing.py= script on them. This outputs
=popsize-distribution-timing-<xxx>.json= files which describe the files that
where used, the estimated minimal truncation parameter and the time associated
with the final evaluation.

** Results

The final step is to generate figures to display the results this is done using
the R scripts in the =src= directory. Don't forget that there is a =shell.nix=
file in the root of this repository to specify the R environment, although there
is nothing fancy so this should run in most R versions.

#+BEGIN_SRC 
Rscript src/plot-profiles.R
Rscript src/plot-llhds.R
#+END_SRC


** Results

Since the whole point of this is the timing of the two methods, lets look first
at how the timings compare. Note that this figure is produced by
=src/plot-profiles.R=.

[[./out/profiles.png]]

But of course, the benefits of a faster algorithm are only meaningful if it
gives the correct results so lets look at a comparison of the LLHD across the
two methods. There appears to be an additive constant that differs between the
two methods, but this wsa also present in Marc's code so I suspect there is
something about numerical stability in his code that accounts for this.

[[./out/llhd-comparison.png]]

Finally, let's consider how the selected truncation parameter differs with the
size of the data set, since this is a novel result too.

[[./out/truncation-comparison.png]]

** Parameters

The parameters used in this computation are defined in a JSON file,
=app-config.json= which is generated by =src/make-json-config.R=.
